{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-p34qayT6Xb",
    "outputId": "ed86312d-e75c-4ab6-a096-4c78ae5afd81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (4.11.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=093de6ec378fd99d37fcbdc29bdb2e6faec73a60549948f71e615ab4c4901676\n",
      "  Stored in directory: /root/.cache/pip/wheels/c2/46/f4/caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OVWKPkvuQwXZ"
   },
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-nZOjm4Qy8R",
    "outputId": "1776512e-7450-4e7f-9927-a8a6c05d77a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation via the off-side rule.Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.Python consistently ranks as one of the most popular programming languages.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wikipedia.summary(\"Python Programming Language\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvbURXsDUCuS",
    "outputId": "b2785b26-1f34-4537-ec18-f1f8ced5b07d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neural network', 'Artificial neural network', 'Recurrent neural network', 'Convolutional neural network', 'Rectifier (neural networks)', 'Feedforward neural network', 'Graph neural network', 'Residual neural network', 'Spiking neural network', 'Neural circuit']\n"
     ]
    }
   ],
   "source": [
    "result = wikipedia.search(\"Neural networks\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XjwSnz6UOhQ",
    "outputId": "41fe8a88-7dbe-4f26-bfd3-d1a82faf302b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WikipediaPage 'Neural network'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the page: Neural network\n",
    "page = wikipedia.page(result[0])\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CvOcnEOCUTml"
   },
   "outputs": [],
   "source": [
    "# get the title of the page\n",
    "title = page.title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oXtrf8PRUZNb"
   },
   "outputs": [],
   "source": [
    "# get the categories of the page\n",
    "categories = page.categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vjkc8woIUeDe"
   },
   "outputs": [],
   "source": [
    "# get the whole wikipedia page text (content)\n",
    "content = page.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Rr9_kI0nUg_V"
   },
   "outputs": [],
   "source": [
    "# get all the links in the page\n",
    "links = page.links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Sl832zKcUj6G"
   },
   "outputs": [],
   "source": [
    "# get the page references\n",
    "references = page.references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "K5Ypt7AnUltY"
   },
   "outputs": [],
   "source": [
    "# summary\n",
    "summary = page.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KseRuErNU4Jq",
    "outputId": "1a31fefc-7236-41ea-bda3-441dafd1c266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page content:\n",
      " A neural network can refer to either a neural circuit of biological neurons (sometimes also called a biological neural network), or a network of artificial neurons or nodes in the case of an artificial neural network. Artificial neural networks are used for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.\n",
      "These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.\n",
      "\n",
      "\n",
      "== Overview ==\n",
      "A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called synapses, are usually formed from axons to dendrites, though dendrodendritic synapses and other connections are possible. Apart from electrical signalling, there are other forms of signalling that arise from neurotransmitter diffusion.\n",
      "Artificial intelligence, cognitive modelling, and neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field, artificial neural networks have been applied successfully to speech recognition, image analysis and adaptive control, in order to construct software agents (in computer and video games) or autonomous robots.\n",
      "Historically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems. Unlike the von Neumann model, neural network computing does not separate memory and processing.\n",
      "Neural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence.\n",
      "\n",
      "\n",
      "== History ==\n",
      "The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.\n",
      "\n",
      "For Bain, every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened. According to his theory, this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs.\n",
      "James's theory was similar to Bain's, however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.\n",
      "C. S. Sherrington (1898) conducted experiments to test James's theory. He ran electrical currents down the spinal cords of rats. However, instead of demonstrating an increase in electrical current as projected by James, Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly, this work led to the discovery of the concept of habituation. \n",
      "Wilhelm Lenz (1920) and Ernst Ising (1925) created and analyzed the Ising model which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982.McCulloch and Pitts  (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.\n",
      "In the late 1940s psychologist Donald Hebb  created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's B-type machines.\n",
      "Farley and Clark (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda (1956).\n",
      "Frank Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit. \n",
      "Some say that neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert (1969). They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. However, by the time this book came out, methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.\n",
      "In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  useful internal representations to classify non-linearily separable pattern classes.Neural network research was boosted when computers achieved greater processing power. Also key in later advances was the backpropagation algorithm. It is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as \n",
      "the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970). The term \"back-propagating errors\" was introduced in 1962 by Frank Rosenblatt, but he did not have an implementation of this procedure, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard.In the late 1970s to early 1980s, interest briefly emerged in theoretically investigating the Ising model by Wilhelm Lenz (1920) and Ernst Ising (1925)\n",
      "in relation to Cayley tree topologies and large neural networks. In 1981, the Ising model was solved exactly for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio and found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.\n",
      "Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function.\n",
      "\n",
      "\n",
      "== Artificial intelligence ==\n",
      "\n",
      "A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.\n",
      "In more practical terms neural networks are non-linear statistical data modeling or decision making tools. They can be used to model complex relationships between inputs and outputs or to find patterns in data.\n",
      "An artificial neural network involves a network of simple processing elements (artificial neurons) which can exhibit complex global behavior, determined by the connections between the processing elements and element parameters. Artificial neurons were first proposed in 1943 by Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, who first collaborated at the University of Chicago.One classical type of artificial neural network is the recurrent Hopfield network.\n",
      "The concept of a neural network appears to have first been proposed by Alan Turing in his 1948 paper Intelligent Machinery in which he called them \"B-type unorganised machines\".The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution, e.g., see the Boltzmann machine (1983), and more recently, deep learning algorithms, which can implicitly learn the distribution function of the observed data. Learning in neural networks is particularly useful in applications where the complexity of the data or task makes the design of such functions by hand impractical.\n",
      "\n",
      "\n",
      "== Applications ==\n",
      "Neural networks can be used in different fields. The tasks to which artificial neural networks are applied tend to fall within the following broad categories:\n",
      "\n",
      "Function approximation, or regression analysis, including time series prediction and modeling.\n",
      "Classification, including pattern and sequence recognition, novelty detection and sequential decision making.\n",
      "Data processing, including filtering, clustering, blind signal separation and compression.Application areas of ANNs include nonlinear system identification and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, data mining (or knowledge discovery in databases, \"KDD\"), visualization and e-mail spam filtering. For example, it is possible to create a semantic profile of user's interests emerging from pictures trained for object recognition.\n",
      "\n",
      "\n",
      "== Neuroscience ==\n",
      "Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems.\n",
      "Since neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.\n",
      "The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).\n",
      "\n",
      "\n",
      "=== Types of models ===\n",
      "Many models are used; defined at different levels of abstraction, and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of the dynamics of neural circuitry arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and its relation to learning and memory, from the individual neuron to the system level.\n",
      "\n",
      "\n",
      "=== Connectivity ===\n",
      "\n",
      "In August 2020 scientists reported that bi-directional connections, or added appropriate feedback connections, can accelerate and improve communication between and in modular neural networks of the brain's cerebral cortex and lower the threshold for their successful communication. They showed that adding feedback connections between a resonance pair can support successful propagation of a single pulse packet throughout the entire network.\n",
      "\n",
      "\n",
      "== Criticism ==\n",
      "Historically, a common criticism of neural networks, particularly in robotics, was that they require a large diversity of training samples for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper \"Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving,\" uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.\n",
      "A. K. Dewdney, a former Scientific American columnist, wrote in 1997, \"Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool.\"Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections—which can consume vast amounts of computer memory and data storage capacity. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons—which must often be matched with incredible amounts of CPU processing power and time. While neural networks often yield effective programs, they too often do so at the cost of efficiency (they tend to consume considerable amounts of time and money).\n",
      "Arguments against Dewdney's position are that neural nets have been successfully used to solve many complex and diverse tasks, such as autonomously flying aircraft.Technology writer Roger Bridgman commented on Dewdney's statements about neural nets: \n",
      "\n",
      "Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\n",
      "In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\n",
      "\n",
      "Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).\n",
      "\n",
      "\n",
      "== Recent improvements ==\n",
      "While initially research had been concerned mostly with the electrical characteristics of neurons, a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine, acetylcholine, and serotonin on behaviour and learning.Biophysical models, such as BCM theory, have been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data.Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful, these efforts could usher in a new era of neural computing that is a step beyond digital computing, because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices.\n",
      "Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning. For example, multi-dimensional long short term memory (LSTM) won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.\n",
      "Variants of the back-propagation algorithm as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto can be used to train deep, highly nonlinear neural architectures, similar to the 1980 Neocognitron by Kunihiko Fukushima, and the \"standard architecture of vision\", inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex.\n",
      "Radial basis function and wavelet networks have also been introduced. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.Deep learning feedforward networks alternate convolutional layers and max-pooling layers, topped by several pure classification layers. Fast GPU-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition and the  ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge. Such neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun and colleagues at NYU.\n",
      "Analytical and computational techniques derived from statistical physics of disordered systems, can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks.\n",
      "\n",
      "\n",
      "== See also ==\n",
      "\n",
      "\n",
      "== References ==\n",
      "\n",
      "\n",
      "== External links ==\n",
      "\n",
      "A Brief Introduction to Neural Networks (D. Kriesel) - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.\n",
      "Review of Neural Networks in Materials Science\n",
      "Artificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid)\n",
      "Another introduction to ANN\n",
      "Next Generation of Neural Networks - Google Tech Talks\n",
      "Performance of Neural Networks\n",
      "Neural Networks and Information\n",
      "Sanderson, Grant (October 5, 2017). \"But what is a Neural Network?\". 3Blue1Brown. Archived from the original on November 7, 2021 – via YouTube. \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Page title:\n",
      " Neural network \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Categories:\n",
      " ['All articles with incomplete citations', 'All articles with unsourced statements', 'Articles with J9U identifiers', 'Articles with LCCN identifiers', 'Articles with hAudio microformats', 'Articles with incomplete citations from April 2019', 'Articles with short description', 'Articles with unsourced statements from January 2022', 'CS1 Finnish-language sources (fi)', 'Computational neuroscience', 'Econometrics', 'Emerging technologies', 'Network architecture', 'Networks', 'Neural networks', 'Short description matches Wikidata', 'Spoken articles', 'Use mdy dates from January 2019'] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Links:\n",
      " ['3Blue1Brown', 'A. K. Dewdney', 'ADALINE', 'Acetylcholine', 'Adaptive control', 'Adaptive resonance theory', 'Adaptive system', 'Alan Turing', 'Alexander Bain (philosopher)', 'Alexey Grigorevich Ivakhnenko', 'Amplitude', 'Analog signal', 'Andrea Walther', 'ArXiv (identifier)', 'Artificial intelligence', 'Artificial neural network', 'Artificial neuron', 'Attention (machine learning)', 'Automatic differentiation', 'Autonomous robot', 'Axon', 'BCM theory', 'Back-propagation', 'Backpropagation', 'Bibcode (identifier)', 'Biological cybernetics', 'Biological neural network', 'Biological neuron models', 'Biologically inspired computing', 'Biophysics', 'Blind signal separation', 'Boltzmann machine', 'Brain connectivity estimators', 'CPU', 'Cerebellar model articulation controller', 'Cerebral cortex', 'Chain rule', 'Charles Scott Sherrington', 'CiteSeerX (identifier)', 'Cognitive architecture', 'Cognitive modelling', 'Cognitive science', 'Computation', 'Computational neuroscience', 'Computer simulation', 'Connectionism', 'Connectomics', 'Control theory', 'Convolution', 'Cultured neuronal networks', 'Data compression', 'Data mining', 'Data modeling', 'Data processing', 'Data storage', 'Database', 'David H. Hubel', 'Decision making', 'Deep Image Prior', 'Deep learning', 'Dendrite', 'Dendrodendritic synapse', 'Digital data', 'Digital morphogenesis', 'Doi (identifier)', 'Donald Hebb', 'Dopamine', 'E-mail spam', 'Efficiently updatable neural network', 'Ernst Ising', 'Evolutionary algorithm', 'Exclusive-or', 'Exclusive or', 'Explainable artificial intelligence', 'Facial recognition system', 'Feedforward neural network', 'Frank Rosenblatt', 'Function approximation', 'GPU', 'Gene expression programming', 'Generative adversarial network', 'Genetic algorithm', 'Geoff Hinton', 'Geoffrey Hinton', 'Gottfried Wilhelm Leibniz', 'Group method of data handling', 'Habituation', 'Handwriting recognition', 'Handwritten text recognition', 'Hebbian learning', 'Henry J. Kelley', 'Herbert Robbins', 'Hopfield network', 'IDSIA', 'ISBN (identifier)', 'ISSN (identifier)', 'Image analysis', 'In situ adaptive tabulation', 'Information processing', 'Information theory', 'Ising model', 'John Hopfield', 'Juergen Schmidhuber', 'Jürgen Schmidhuber', 'Knowledge representation', 'Kunihiko Fukushima', 'Learning', 'Long short term memory', 'Long term potentiation', 'Machine learning', 'Marvin Minsky', 'Mathematical model', 'Memristor', 'Multilayer perceptrons', 'NYU', 'Nanodevice', 'Nature Nanotechnology', 'Nature Neuroscience', 'Neocognitron', 'Neural Computation (journal)', 'Neural backpropagation', 'Neural circuit', 'Neural computing', 'Neural network (disambiguation)', 'Neural network software', 'Neural networks', 'Neuromodulators', 'Neuromorphic computing', 'Neuron', 'Neurotransmitter', 'Node (computer science)', 'Non-linear', 'Nonlinear system identification', 'Novelty detection', 'OCLC (identifier)', 'PMC (identifier)', 'PMID (identifier)', 'Parallel constraint satisfaction processes', 'Parallel distributed processing', 'Pattern recognition', 'Paul Werbos', 'Perceptron', 'Phase transition', 'Predictive analytics', 'Predictive modeling', 'Principal component', 'Programming language', 'Pyramidal neuron', 'Radial basis function network', 'Radial basis networks', 'Random-access memory', 'Recurrent neural network', 'Regression analysis', 'Reverse accumulation', 'Roger Bridgman', 'S2CID (identifier)', 'Scientific American', 'Self-organizing map', 'Seppo Linnainmaa', 'Serotonin', 'Seymour Papert', \"Shun'ichi Amari\", 'Simulated reality', 'Software agents', 'Speech recognition', 'Statistical', 'Statistical classification', 'Stochastic gradient descent', 'Support vector machine', 'Symbolic artificial intelligence', 'Synapse', 'Synaptic plasticity', 'Threshold logic', 'Time delay neural network', 'Time series prediction', 'Tomaso Poggio', 'Torsten Wiesel', 'University of Chicago', 'University of Toronto', 'Unorganized machine', 'Unsupervised learning', 'Video game', 'Visual cortex', 'Von Neumann model', 'Walter Pitts', 'Warren McCulloch', 'Warren Sturgis McCulloch', 'Wilhelm Lenz', 'William James', 'Yann LeCun', 'YouTube'] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "References:\n",
      " ['http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/4', 'http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks', 'http://www.dkriesel.com/en/science/neural_networks', 'http://members.fortunecity.com/templarseries/popper.html', 'http://werbos.com/Neural/SensitivityIFIPSeptember1981.pdf', 'http://uhaweb.hartford.edu/compsci/neural-networks-definition.html', 'http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf', 'http://www.nasa.gov/centers/dryden/news/NewsReleases/2003/03-49.html', 'http://uli.nli.org.il/F/?func=find-b&local_base=NLX10&find_code=UID&request=987007553827205171', 'http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions', 'http://arxiv.org/archive/cs.NE', 'http://www.msm.cam.ac.uk/phase-trans/2009/performance.html', 'http://www.msm.cam.ac.uk/phase-trans/2009/review_Bhadeshia_SADM.pdf', 'http://www.msm.cam.ac.uk/phase-trans/abstracts/neural.review.html', 'https://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf', 'https://people.lu.usi.ch/mascij/data/papers/2012_nn_traffic.pdf', 'https://www.gartner.com/it-glossary/neural-net-or-neural-network', 'https://books.google.com/books?id=FhwVNQAACAAJ', 'https://books.google.com/books?id=bOIGAAAAYAAJ&q=leibniz+altered+manuscripts&pg=PA90', 'https://books.google.com/books?id=rGFgAAAAMAAJ', 'https://books.google.com/books?id=xoiiLaRxcbEC', 'https://medicalxpress.com/news/2020-08-neuroscientists-regions-brain.html', 'https://www.youtube.com/watch?v=AyzOUbkUf3M', 'https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi', 'https://ui.adsabs.harvard.edu/abs/1967RvMP...39..883B', 'https://ui.adsabs.harvard.edu/abs/1982PNAS...79.2554H', 'https://ui.adsabs.harvard.edu/abs/2008Natur.453...80S', 'https://ui.adsabs.harvard.edu/abs/2020PLSCB..16E8033R', 'https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.4502', 'https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.588.3775', 'https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.76.1541', 'https://id.loc.gov/authorities/subjects/sh93002348', 'https://pubmed.ncbi.nlm.nih.gov/10526343', 'https://pubmed.ncbi.nlm.nih.gov/13602029', 'https://pubmed.ncbi.nlm.nih.gov/16764513', 'https://pubmed.ncbi.nlm.nih.gov/18451858', 'https://pubmed.ncbi.nlm.nih.gov/18654568', 'https://pubmed.ncbi.nlm.nih.gov/19299860', 'https://pubmed.ncbi.nlm.nih.gov/32776924', 'https://pubmed.ncbi.nlm.nih.gov/6953413', 'https://pubmed.ncbi.nlm.nih.gov/7370364', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7444537', 'https://www.researchgate.net/publication/328964756', 'https://archive.org/details/in.ernet.dli.2015.226341', 'https://archive.org/details/paralleldistribu00rume', 'https://archive.org/details/principlespsych01jamegoog', 'https://web.archive.org/web/20090318133122/http://www.gc.ssr.upm.es/inves/neural/ann1/anntutorial.html', 'https://web.archive.org/web/20091216110504/http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html', 'https://web.archive.org/web/20120319163352/http://members.fortunecity.com/templarseries/popper.html', 'https://web.archive.org/web/20140529155320/http://uhaweb.hartford.edu/compsci/neural-networks-definition.html', 'https://web.archive.org/web/20160414055503/http://werbos.com/Neural/SensitivityIFIPSeptember1981.pdf', 'https://web.archive.org/web/20180831075249/http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions', 'https://arxiv.org/abs/2212.11279', 'https://creativecommons.org/licenses/by/4.0/', 'https://doi.org/10.1007%2FBF00344251', 'https://doi.org/10.1007%2FBF02478259', 'https://doi.org/10.1007%2Fbf01931367', 'https://doi.org/10.1016%2F0378-4371(83)90138-3', 'https://doi.org/10.1016%2F0378-4371(83)90157-7', 'https://doi.org/10.1037%2Fh0042519', 'https://doi.org/10.1038%2F14819', 'https://doi.org/10.1038%2Fnature06932', 'https://doi.org/10.1038%2Fnnano.2008.160', 'https://doi.org/10.1073%2Fpnas.79.8.2554', 'https://doi.org/10.1098%2Frstb.1898.0002', 'https://doi.org/10.1103%2FRevModPhys.39.883', 'https://doi.org/10.1109%2FTIT.1954.1057468', 'https://doi.org/10.1109%2FTIT.1956.1056810', 'https://doi.org/10.1109%2FTPAMI.2008.137', 'https://doi.org/10.1162%2Fneco.2006.18.7.1527', 'https://doi.org/10.1214%2Faoms%2F1177729586', 'https://doi.org/10.1371%2Fimage.pcbi.v06.i08', 'https://doi.org/10.1371%2Fjournal.pcbi.1008033', 'https://doi.org/10.2514%2F8.5282', 'https://doi.org/10.3233%2F978-1-61499-894-5-179', 'https://ghostarchive.org/varchive/youtube/20211107/aircAruvnKk', 'https://api.semanticscholar.org/CorpusID:122357351', 'https://api.semanticscholar.org/CorpusID:12781225', 'https://api.semanticscholar.org/CorpusID:14635907', 'https://api.semanticscholar.org/CorpusID:15568746', 'https://api.semanticscholar.org/CorpusID:206775608', 'https://api.semanticscholar.org/CorpusID:221100528', 'https://api.semanticscholar.org/CorpusID:2309950', 'https://api.semanticscholar.org/CorpusID:4367148', 'https://api.semanticscholar.org/CorpusID:8920227', 'https://www.wikidata.org/wiki/Q55624999#identifiers', 'https://www.worldcat.org/issn/1553-7358', 'https://www.worldcat.org/oclc/35558945', 'https://www.worldcat.org/oclc/8231704'] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Summary:\n",
      " A neural network can refer to either a neural circuit of biological neurons (sometimes also called a biological neural network), or a network of artificial neurons or nodes in the case of an artificial neural network. Artificial neural networks are used for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.\n",
      "These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print info\n",
    "print(\"Page content:\\n\", content, \"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"Page title:\\n\", title, \"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"Categories:\\n\", categories, \"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"Links:\\n\", links, \"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"References:\\n\", references, \"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"Summary:\\n\", summary, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDys7d4UU7h_",
    "outputId": "a8e8e879-8835-4cac-97ba-a4dccf29d4ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of web scraping in es: Web scraping o raspado web, es una técnica utilizada mediante programas de software para extraer información de sitios web.[1]​ Usualmente, estos programas simulan la navegación de un humano en la World Wide Web ya sea utilizando el protocolo HTTP manualmente, o incrustando un navegador en una aplicación.\n",
      "El web scraping está muy relacionado con la indexación de la web, la cual indexa la información de la web utilizando un robot y es una técnica universal adoptada por la mayoría de los motores de búsqueda. Sin embargo, el web scraping se enfoca más en la transformación de datos sin estructura en la web (como el formato HTML) en datos estructurados que pueden ser almacenados y analizados en una base de datos central, en una hoja de cálculo o en alguna otra fuente de almacenamiento. Alguno de los usos del web scraping son la comparación de precios en tiendas, la monitorización de datos relacionados con el clima de cierta región, la detección de cambios en sitios webs y la integración de datos en sitios webs. También es utilizado para obtener información relevante de un sitio a través de los rich snippets. \n",
      "En los últimos años el web scraping se ha convertido en una técnica muy utilizada dentro del sector del posicionamiento web gracias a su capacidad de generar grandes cantidades de datos para crear contenidos de calidad.[2]​\n"
     ]
    }
   ],
   "source": [
    "# changing language\n",
    "# for a list of available languages, \n",
    "# check http://meta.wikimedia.org/wiki/List_of_Wikipedias link.\n",
    "language = \"es\"\n",
    "wikipedia.set_lang(language)\n",
    "# get a page and print the summary in the new language\n",
    "print(f\"Summary of web scraping in {language}:\", wikipedia.page(\"Web Scraping\").summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrvYiGVGVhYN",
    "outputId": "305e9ea3-9c9a-4987-a785-31882b2c0ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of web scraping in my: ဗိုလ်ချုပ်မှူးကြီး သတိုးမဟာသရေစည်သူ သတိုးသီရိသုဓမ္မ မင်းအောင်လှိုင်သည် မြန်မာနိုင်ငံ၏ တပ်မတော်သား တစ်ဦးဖြစ်ပြီး၊ တပ်မတော်ကာကွယ်ရေးဦးစီးချုပ် ရာထူးနေရာ၌ လက်ရှိ တာဝန်ထမ်းဆောင်နေသူဖြစ်သည်။ ၂၀၂၁ ခုနှစ် ဖေဖော်ဝါရီလ ၁ ရက်နေ့တွင် စစ်အာဏာသိမ်းပြီးနောက် နိုင်ငံတော်စီမံအုပ်ချုပ်ရေးကောင်စီ ဥက္ကဋ္ဌ အဖြစ် တာဝန်ထမ်းဆောင်လျက်ရှိသည်။ ၂၀၂၁ ခုနှစ် ဩဂုတ်လ ၁ ရက်နေ့တွင် ဖွဲ့စည်းခဲ့သော အိမ်စောင့်အစိုးရတွင်လည်း ဝန်ကြီးချုပ်အဖြစ် မိမိကိုယ်ကိုပြန်လည်ခန့်အပ်ကာ တာဝန် ထမ်းဆောင်နေသည်။ ယင်းဖန်တီးထားသော အိမ်စောင့်အစိုးရ၏ အစိုးရအကြီးအကဲအဖြစ် အုပ်ချုပ်ရေးအာဏာနှင့် ကောင်စီဥက္ကဋ္ဌအဖြစ် ဥပဒေပြုရေးအာဏာတို့ကို ကိုယ်တိုင်ကျင့်သုံးလျက်ရှိသည်။ \n",
      "သူသည် တပ်မတော်ကာကွယ်ရေးဦးစီးချုပ် ရာထူးကို ၂၀၁၁ ခုနှစ် မတ်လ ၃၀ ရက်နေ့တွင် လက်ခံခဲ့သည်။ သူသည် နိုင်ငံတော်သမ္မတ ဦးဆောင်သော အဖွဲ့ဝင် (၁၁) ဦးပါဝင်သည့် အမျိုးသားကာကွယ်ရေးနှင့်လုံခြုံရေးကောင်စီ၏ (NDSC) ၏အဖွဲ့ဝင်လည်းဖြစ်သည်။ သူသည် ယခင်က မြန်မာနိုင်ငံကာကွယ်ရေးဝန်ကြီးဌာန၏ ညှိနှိုင်းကွပ်ကဲရေးမှူး (ကြည်း၊ရေ၊လေ) ဖြစ်သည်။ရခိုင်ပြည်နယ်ရှိ ရိုဟင်ဂျာလူမျိုးတုန်းသတ်ဖြတ်မှုအတွက် အဓိကတာဝန်ရှိသူလည်းဖြစ်သည်။ နိုင်ငံခြားရေးမူဝါဒတွင် သူသည် အာဆီယံ၏လွှမ်းမိုးမှုကို တွန်းလှန်ပြီး ရုရှား၊ တရုတ်၊ အိန္ဒိယနိုင်ငံတို့အပေါ် ပိုမိုအားကိုးခဲ့သည်။ သူ၏ လူ့အခွင့်အရေး ချိုးဖောက်မှုများနှင့် အကျင့်ပျက်ခြစားမှုများကြောင့် နိုင်ငံတကာ၏ အရေးယူပိတ်ဆို့မှုများ ဆက်တိုက်ချမှတ်ခံခဲ့ရလျက်ရှိသည်။ ၂၀၂၂ ခုနှစ် ဒီမိုကရေစီ အညွှန်းကိန်း၏ အဆင့်သတ်မှတ်ချက်များအရ သူ၏လက်‌ထက်မြန်မာနိုင်ငံသည် အာဖဂန်နစ္စတန်ပြီးလျှင် ဒုတိယအာဏာရှင်အဆန်ဆုံးနိုင်ငံဖြစ်ခဲ့သည်။ ၂၀၂၃ ဧပြီလ ၁၃ တွင် တိုင်းမ်စ်မဂ္ဂဇင်းမှ သူ့အား \"၂၀၂၃ ခုနှစ်၏ သြဇာအရှိဆုံး လူပုဂ္ဂိုလ် ၁၀၀\" ထဲတွင် ထည့်သွင်းခဲ့ပြီး \"နိုင်ငံကိုအပယ်ခံဘဝသို့ ရောက်စေခဲ့သည်\" ဟူ၍ဖော်ပြထားသည်။\n"
     ]
    }
   ],
   "source": [
    "language = \"my\" \n",
    "wikipedia.set_lang(language)\n",
    "# get a page and print the summary in the new language\n",
    "print(f\"Summary of web scraping in {language}:\", wikipedia.page(\"မင်းအောင်လှိုင်\").summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC3lg5YIV_Cl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
